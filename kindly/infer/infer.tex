\section{Inference}
\label{inference}

An important contribution of \affe is its principal type inference.
We now formulate our inference algorithm
based on the $HM(X)$ framework~\citep{DBLP:journals/tapos/OderskySW99}.
$HM(X)$ is a Hindley-Milner type system for a language
with qualified types where constraints are expressed in an arbitrary
theory $X$.
Most importantly,
$HM(X)$ ensures principal type inference
when $X$ respects the specified properties.
We apply HM(X) to a concrete constraint language which we name $\CL$.
We adapt and extend HM(X)'s existing rules to support kind inference,
track linearity and handle borrows and regions. Finally, we
formulate an appropriate constraint solving algorithm for $\CL$
which solves and simplifies constraints in a principled way.
We start by various preliminary definitions.

\subsection{Preliminaries}

Unlike the syntax-directed version of our type system, knowing which elements
are input and output or our inference judgements is critical. In the rest
of this section, when presenting a new judgement,
we write input parameters in \textbf{\textcolor{DarkSlateBlue}{bold blue}}. The rest will then be
output parameters.

\subsubsection{Usage environments}

% One novelty of our inference  judgement now
% returns a type environment, which we call ``usage environment'' and commonly
% write $\Sv$, to summarize how variables and borrows are used inside
% the expression.

In order to determine if a variable is used in an affine manner, we must track
its uses and the associated kinds. For instance, in the expression
$(x,x)$, $x$ is used twice. If $x$ is of type $\tau$, which is of kind $k$,
we must add the constraint $\Cleq{k}{\kun}$.
%
In order to infer such constraints, our inference judgement will not only
take an environment as parameter but also return an environment which
we call ``usage environment'' and which summarizes how variables and borrows
are used. Usage environments follow the exact same grammar
as normal environments. In order to distinguish them more easily,
we denote them by $\Sv$.

In \cref{sdtyping}, we used relations to split environments in two and to
transform suspended bindings into borrows inside a region.
These relations take as argument a constraint which validates
the transformations.
In the context of inference, we define new judgements which \emph{infer}
the constraints.
\begin{itemize}[leftmargin=*]
\item $\bsplit{C}{\Sv}{\inP{\Sv_1}}{\inP{\Sv_2}}$.
  Given two usage environments $\inP{\Sv_1}$ and $\inP{\Sv_2}$
  (inferred from two subexpressions, for instance),
  we return $\Sv$, the merged environment, and $C$, a set
  of constraints that must be respected.
  This relation is total and non-ambiguous given $\Sv_1$ and $\Sv_2$
  and uses the same rules as the one presented in \cref{sdtyping:split}.
\item $\bregion[\inP{n}]{C}{\inP{x}}{\Sv}{\inP\Sv'}$.
  Given a usage environment $\inP{\Sv'}$, a nesting level $\inP{n}$,
  and a variable name $\inP{x}$, we return
  $\Sv$ where the borrow binding of $x$ in $\Sv'$, if it exists,
  is replaced by
  a suspended binding. We also return $C$, a set of constraints that must
  be respected.
  Again, the relation is total and non-ambiguous for any given $\Sv'$,
  $n$ and $x$,
  and uses the rules presented in \cref{sdtyping:regions}.
\end{itemize}

The relations used for syntax-directed typing can trivially be defined
in terms of these new relations by using constraint entailment.
All relations are fully described in \cref{typ:extra:envs}.

\subsubsection{Constraint normalization}

The HM(X) framework assumes the existence of a ``$\operatorname{normalize}$''
function which takes a constraint $C$, a substitution $\psi$ and returns a
simplified constraint $C'$
and an updated substitution $\unif'$.
Normalization must return a normal form such that $\unif'$ is a most general unifier.
We detail the implementation
of the normalization function and its property in \cref{infer:solving}.
In the meantime, we simply
assume the existence of such a function for our constraint system.

\subsection{Type inference}

We write $\inferW{\Sv}{(C,\unif)}{\inP{\E}}{\inP{e}}{\tau}$ when
$\inP{e}$\ has type $\tau$ in $\inP{\E}$ under the constraints $C$ and unifier $\unif$
with a usage environment $\Sv$. $\inP\E$ and $\inP{e}$ are the input parameters of our
inference algorithm.
Unlike in the syntax-directed version, $\E$ only regular and type bindings.
Suspended and borrow bindings can only be present in $\Sv$.
We revisit some of the syntax-directed rules presented in \cref{sdtyping}
to highlight the novelties
of our inference algorithm and the differences with the syntax-directed
system.
As before, the complete type inference rules are shown in \cref{appendix:infer}.

\subsubsection{Environments and bindings}
\label{infer:envs}
%
\begin{figure*}[tb]
  \begin{mathpar}
    \ruleIVar
    \and
    \ruleIAbs
    \and
    \ruleILet
    \hfill
    \ruleIRegion
    \and
    \ruleIPair
    \and
    \inferrule{}
    { \Weaken_{\bvar{x}{\sigma}}(\Sv) =
      \text{if } (x\in\Sv) \text{ then} \Ctrue \text{else }
      \Cleq{\sigma}{\kaff_\infty}
      % \begin{dcases}
      %   \Ctrue&\text{if } x\in\Sv\\
      %   \Cleq{\sigma}{\kaff_\infty}&\\
      % \end{dcases}
      \\\\\\
      \generalize{C}{\E}{\tau} =
      (\Cproj{(\Multi{\kvar},\Multi{\tvar})}{C},
      \forall (\Multi{\kvar},\Multi{\tvar}).\qual{C}{\tau})\\
      \text{ where}\\\\
      \Multi{\kvar},\Multi{\tvar} = (\fv{\tau}\cup\fv{C})\setminus\fv{\E}
    }
  \end{mathpar}
  \vspace{-15pt}
  \caption{Selected inference rules}
  \label{rule:infer:envs}
  \label{rule:infer:envrules}
  \label{rule:infer:let}
\end{figure*}
%
In our syntax-directed system, we ensured
that linear variables are not discarded at the \emph{leaves}, through
the {\sc Var} rule. In the inference algorithm, we operate in the opposite
direction: we collect data from the leaves, and only enforce linearity
at \emph{binders}. This is reflected in the {\sc Var$_I$} and
{\sc Abs$_I$} rules presented in \cref{rule:infer:envs}.
Typing for variables is very similar to traditional (non-affine) ML
typing. The only difference is that we collect
the fact that $x$ was used with the scheme $\schm$ by returning
a usage environment $\Sv = \{ \bvar{x}{\schm} \}$.
%
This usage environment is in turn used at each binder to enforce proper
usage of linear variable via the $\Weaken$ property.
We demonstrate how this is achieved for lambda expressions
in the {\sc Abs$_I$} rule.
First, we typecheck the body of the lambda and obtain a usage
environment $\Sv_x$. Just like in the syntax-directed type system,
we introduce the constraint
$\Cleq{\Sv\Sdel{x}}{\kvar}$ which properly accounts for captures in
the body of the lambda expression. We also introduce the constraint
$\Weaken_{\bvar{x}{\sigma}}(\Sv)$, which is false only if weakening
is necessary but impossible due to linearity.
The $\Weaken$ constraint is introduced at each binding point in the program
such as let-bindings and pattern matching.

Finally, we use normalization on complex constraints to ensure
that the inference algorithm always return the simplest possible
constraints and unifiers.



\subsubsection{Splitting and Regions}
\label{infer:split}
\label{infer:regions}

We now present the inference version
of the {\sc Pair} and {\sc Region} rules in \cref{rule:infer:envrules}.
While the rules are quite similar to the original version, the usage
environment $\Sv$ is now an \emph{output} of the algorithm.
As such, we use the ``inference'' version of the relations on
the environment,
$\bsplit{C}{\Sv}{\inP{\Sv_1}}{\inP{\Sv_2}}$
and $\bregion{C}{\inP{x}}{\Sv}{\inP\Sv'}$,
which returns the necessary constraints.
As appropriate for inference, these relations
are total and non ambiguous for their input parameters ($\Sv_1, \Sv_2$ and $\Sv'$, respectively).
We then collect all constraints and normalize them.

\subsubsection{Generalization and constraints}

So far, constraints have only been composed of a list of kind inequalities.
The internal language of constraints used for inference is in fact
richer.
To illustrate this, we show the inference rule for let-bindings
in \cref{rule:infer:let}.
This rule combines several ingredients previously seen:
since let expressions are binders, we use $\Weaken$ on the bound
identifier $x$. Since let-expressions contain
two subexpressions, we use the environment splitting relation,
$\bsplit{C_s}{\Sv}{\Sv_1}{(\Sv_2 \Sdel{x})}$. We remove the $x$ from
the right environment, since it is not available outside of the expression
$e_2$, and should not be part of the returned usage environment.

As per tradition in ML languages, generalization is performed
on let-bindings.
Following HM(X), we write $(C_\schm, \schm) = \generalize{C}{\E}{\tau}$
for the pair of a constraint and a scheme resulting from
generalization. The definition is provided in \cref{rule:infer:let}.
The type scheme $\schm$ is created by quantifying over all the appropriate
free variables and the current constraints.
The generated constraint $C_\schm$ uses a new projection operator,
$\Cproj{x}{D}$ where $x$ can be either a type or a kind variable, which
allow the creation of local variables inside the constraints.
This allows us to encapsulate all the quantified variables in the global constraints.
It also reflects the fact that there
should exist at least one solution for $C$ for the scheme to be valid.
\citet{DBLP:journals/tapos/OderskySW99} give a detailed account
on the projection operators in HM inference.


\subsection{Constraint solving}
\label{infer:solving}

\newcommand\A{\mathcal A}
\newcommand\SC{\mathcal S}

We define the constraint solver in terms of an arbitrary commutative bounded
lattice $(\mathcal L, \lk_\Lat)$, i.e.,
a lattice which has a minimal and a maximal element ($l^\top$ and $l^\bot$)
and where meet and joins are commutative.
We write lattice elements as $l$ and $\glb_i l_i$ (resp. $\lub_i l_i$)
for the greatest lower bound (resp. least upper bound) in $\mathcal L$.
The lattice for \lang (see \cref{sdtyping}) is a bounded lattice with
$l^\top = \klin_\infty$ and $l^\bot = \kun_0$.

Let $\CL$ be the set of constraints in such a lattice $\mathcal L$.
The full grammar of constraints is shown in \cref{grammar:constraint}.
Constraints are made of kind inequalities, conjunctions and
projections, as shown previously, along with type unification
constraints. Since types might contain kinds (for instance, on the arrows),
type unification is oriented and written as $\leq$.
For simplicity, we consider all type constructors
invariant in their parameters
and define $\Ceq{\tau}{\tau'}$ as $\Cleq{\tau}{\tau'} \wedge \Cleq{\tau'}{\tau}$.

As before, entailment is denoted by $\entail{C}{D}$,
where $D$ is a consequence of the constraints $C$.
We say that $C$ and $D$ are equivalent, $C \equivC D$,
when $\entail{C}{D}$ and $\entail{D}{C}$.
The base rules for entailment are given in \cref{rules:entail}.
The rules for classic Herbrand unification and
projection operators are omitted for brevity (see
\citep{DBLP:journals/tapos/OderskySW99} for
details).

\begin{figure*}[!bt]
  \centering
  \begin{minipage}{0.32\linewidth}
    \begin{align*}
      C &::= \Cleq{\tau_1}{\tau_2} \\
        &\mid \Cleq{k_1}{k_2} \tag{Inequalities} \\
        &\mid C_1 \Cand C_2  \tag{Conjunction}\\
        &\mid \Cproj{\tvar}{C}
        \mid \Cproj{\kvar}{C} \tag{Projection}
    \end{align*}
    \caption{The constraint language}
    \label{grammar:constraint}
  \end{minipage}\hfill
  \begin{minipage}{0.66\linewidth}
    \begin{mathpar}
      \inferrule
      {\forall i,\ \entail{C}{\Cleq{l_i}{k}}}
      {\entail{C}{\Cleq{\wedge_i l_i}{k}}}
      \and
      \inferrule
      {\forall i,\ \entail{C}{\Cleq{k}{l_i}}}
      {\entail{C}{\Cleq{k}{\vee_i l_i}}}
      \and
      \inferrule
      { \forall i,\ \entail{C}{\Ceq{\tau_i}{\tau_i}}\\
      }
      { \entail{C}{\Cleq{\tapp{\tcon}{(\tau_i)}}{\tapp{\tcon}{(\tau'_i)}}} }
      \and
      \inferrule{l \lk_\Lat l'}{\entail{}{\Cleq{l}{l'}}}
      \and
      \inferrule
      { \entail{C}{\Cleq{\tau'_1}{\tau_1}}\\
        \entail{C}{\Cleq{\tau_2}{\tau'_2}}\\
        \entail{C}{\Cleq{k}{k'}}
      }
      { \entail{C}{\Cleq{\tau_1\tarr{k}\tau_2}{\tau'_1\tarr{k'}\tau'_2}} }
    \end{mathpar}
    \caption{Base entailment rules -- $\entail{\inP{C}}{\inP{D}}$ }
    \label{rules:entail}
  \end{minipage}
\end{figure*}

We define the set of solved formed
$\mathcal S$ as the quotient set of $\CL$ by $\equivC$.
We will show later that such constraints are in fact only composed of
kind inequalities, and thus correspond to the syntactic constraints
used in type and kind schemes.
%
We now define our normalization procedure $\normalize{C_0}{\unif_0}$, where
$C_0\in \CL$ is a set of constraints and $\unif_0$ is a substitution.
It returns a constraint $C \in \mathcal S$ in
solved form and a unifier $\unif$.
The main idea of the algorithm is to first remove all the type equalities
by using regular Herbrand unification. After that, we only have
a set of inequalities among kinds, which we can consider as a relation.
We can then saturate the relation,
unify all kinds that are in the same equivalence classes to obtain
a most general unifier on kind variables,
remove all existentially quantified variables and
then minimize back the relation and apply various
simplification rules to make the resulting type easier to understand to users.

More precisely, we apply the following steps:
\begin{enumerate}
\item Solve all type equality constraints through Herband unification and
  gather all existential quantifications at the front of the constraint.
  We obtain a constraint $C^k = \exists \Multi\kvar,\ \Cleq{k_j}{k'_j}_j$ and
  a substitution $\unif_\tau$.

  We write $\mathcal R$ for the relation $\Cleq{k_j}{k'_j}_j$,
  $\mathcal G$ the underlying directed graph and $V$ its vertices.

\item Saturate the lattice equalities in $\mathcal R$.

  More precisely, for each kind variable $\kvar \in V$,
  for each constant $l_i$ (resp. $l_j$) such that
  there is a path from $l_i$ to $\kvar$ (resp. from $\kvar$ to $l_j$) in $\mathcal G$,
  add an edge from $\lub l_i$ to $\kvar$
  (resp. from $\kvar$ to $\glb l_j$).
  This step is well defined since $\mathcal L$ is a bounded lattice
  and $\lub\emptyset$ and $\glb\emptyset$ are well defined.

  We also complement $\mathcal R$ with $(\leq)$ by adding an edge
  between related constants.
\item
  At this point, we can easily check for satisfiability: A constraint
  is satisfiable (in the given environment) if and only if,
  for any constants $l_1$ and $l_2$ such that
  there is a path from $l_1$ to $l_2$ in $\mathcal G$, then $l_1\lk_\Lat l_2$.
  If this is not the case, we return \textbf{fail}.

\item For each strongly connected component in $\mathcal G$, unify all its vertices and replace it by a representative.
  We write $\unif_k$ for the substitution that replaces a kind variable by
  its representative.
  The representative of a strongly connected component $g$ can be determined as follows:
  \begin{itemize}
  \item If $g$ does not contain any constant, then the representative
    is a fresh kind variable.
  \item If $g$ contains exactly one constant, it is the representative.
  \item Otherwise, the initial constraint $C_0$ is not satisfiable.
  \end{itemize}
  Note that this step will also detect all unsatisfiable constraints.
\item Take the transitive closure of $\mathcal R$.
\item Remove all the vertices corresponding to the kind variables $\Multi\kvar$
  that are existentially quantified in $C^k$.
\item Take the transitive reduction of $\mathcal R$.
\item Remove the extremums of $\mathcal L$ and the edges of $(\leq)$
  from $\mathcal R$.
\item Return $C = \left\{ k \leq k' \mid k \operatorname{\mathcal R}k' \right\}$
  and $\unif =  \unif_\tau \meet \unif_k$.
\end{enumerate}

An example of this algorithm in action is shown in \cref{solving:example}.
Our algorithm is complete, computes principal normal forms,
and already simplifies constraints significantly
(thanks to steps 6, 7 and 8).
It can be extended with further simplification phases.
In particular, our implementation and all the signatures presented in
\cref{motivation} use a variance-based simplification
where all covariant (resp. contravariant) variables are replaced by their
lower (resp. upper) bounds.
All the simplification mechanisms presented
here, including the variance-based one, are complete.
It is also possible to add ``best-effort'' simplification
rules which help reduce the size of inferred signatures even further
\citep{DBLP:conf/aplas/Simonet03}.
%
We now show that this algorithm is well-behaved with respect to entailment
and computes unique normal forms.

\begin{property}[Principal normal form]
  Normalization computes principal normal forms for $\CL$, i.e.
  given a constraint $D\in\CL$, a substitution $\phi$ and
  $(C,\unif) = \normalize{D}{\phi}$,
  then $\phi\leq\unif$,
  $C \equivC \unif D$ and
  $\unif C = C$.
\end{property}

\begin{property}[Principal constraints]
  $\CL$ has the principal constraint property, i.e.
  for every $D\in\CL$, and a substitution $\unif$,
  either $D$ does not have a normal form, or it has
  a principal normal form.
\end{property}

\begin{property}[Regular constraint system]
  $\CL$ is regular, ie, for $x, x'$ two types or kinds,
  $\entail{}{\Ceq{x}{x'}}$ implies
  $\fv{x} = \fv{x'}$
\end{property}

The proofs are given in \cref{appendix:constraints}. These three properties
are sufficient to state that $HM(\CL)$ provides principal type inference.
However, we use an extension of HM(X) with kind inference
and affine types and borrows.
We now show that HM(X)'s completeness and soundness theorems are still valid on
this extended system.

\subsection{Soundness and Principality}

We now show that our extended inference algorithm still satisfies the soundness
and completeness properties of HM(X).
% This is achieved by extending
% the original proofs from HM(X), which is done in \cref{appendix:infer}.
%
The first theorem states that our inference algorithm is sound
with respect to the syntax-directed type system.

\begin{theorem}[Soundness of inference]
  Given a type environment $\E$ containing only value bindings,
  $\E|_\tau$ containing only type bindings, and a term $e$:\\
  if $\inferW{\Sv}{(C,\unif)}{\E;\E_\tau}{e}{\tau}$\\
  then $\inferS{C}{\unif(\Sv;\E_\tau)}{e}{\tau}$, $\unif C = C$ and $\unif \tau = \tau$
\end{theorem}

The syntax-directed derivation holds with the usage environment $\Sv$ instead of the originally provided environment $\E$. Indeed,
$\E$ does not contain suspended and borrow bindings, since those
are discovered on the fly and recorded in $\Sv$. The type binding, however,
are directly taken from the syntax-directed derivation.

The second theorem states that our algorithm is complete: for any given
syntax-directed typing derivation, our inference algorithm can find
a derivation that gives a type at least as general.
For this, we need first to provide a few additional definitions.

\begin{definition}[More general unifier]
  Given a set of variable $U$ and $\unif$, $\unif'$ and $\phi$
  substitutions. \\
  Then
  $\unif \leq^{\phi}_{U} \unif'$ iff $(\phi \circ \unif)|_{U} = \unif'|_U$.
\end{definition}

\begin{definition}[Instance relation]
  Given a constraint $C$ and two schemes
  $\schm = \forall \Multi\tvar. \qual{D}{\tau}$ and
  $\schm' = \forall \Multi\tvar'. \qual{D'}{\tau'} $.
  Then $\entail{C}{\schm \preceq \schm'}$
  iff $\entail{C}{\subst{\tvar}{\tau''} D}$
  and $\entail{C\Cand D'}{\Cleq{\subst{\tvar}{\tau''}\tau}{\tau'}}$
\end{definition}

\begin{definition}[Flattened Environment]
A flattened environment,
written as $\Eflat\E$, is the environment
where all the binders are replaced by normal ones. More formally:
\begin{align*}
  \Eflat\E
  =& \left\{\bvar{x}{\tau}\in\E \mid
    \vee \bvar{\borrow{x}}{\borrowty{k}{\tau}}\in\E
    \vee \svar{x}{\tau}^n\in\E
    \right\}\\
  &\cup \left\{ \bvar{\tvar}{k} \mid \bvar{\tvar}{k}\in\E \right\}
\end{align*}
\end{definition}


We can then state the completeness theorem.

\begin{theorem}[Completeness]
  Given $\inferS{C'}{\E'}{e}{\tau'}$ and
  $\entail{}{\unif'\E \preceq \E'}$.
  Then $$\inferW{\Sv}{(C,\unif)}{\Eflat\E}{e}{\tau}$$
  for some environment $\Sv$,
  substitution $\unif$, constraint $C$ and type $\tau$ such
  that
  \begin{align*}
    \unif &\leq^{\phi}_{\fv{\E}} \unif'
    &\entail{C'&}{\phi C}
    &\entail{&}{\phi \schm \preceq \schm'}
    &\Sv&\subset\E
    % &( C, \schm, \unif) &\leq (C',\schm',\unif')
  \end{align*}
  where $\schm' = \generalize{C'}{\E'}{\tau'}$
  and $\schm = \generalize{C}{\E}{\tau}$
\end{theorem}

Finally, principality is a direct application of completeness to
toplevel programs.

\begin{corollary}[Principality]
  Let $\inferS{\Ctrue}{\E}{e}{\schm}$ a closed typing judgement.\\
  Then $\inferW{\Sv}{(C,\unif)}{\Eflat\E}{e}{\tau}$
  such that:
  \begin{align*}
    (\Ctrue,\schm_o) &= \generalize{C}{\unif\E}{\tau}
    &\entail{&}{\schm_o \preceq \schm}
    % &( C, \schm, \unif) &\leq (C',\schm',\unif')
  \end{align*}


\end{corollary}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
